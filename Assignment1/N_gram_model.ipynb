{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Assignment-1 (N-gram model)</center></h3>\n",
    "<center>Name - Bbiswabasu Roy<center>\n",
    "<center>Roll - 19EC39007<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename, N):\n",
    "    with open(filename, 'r') as f:\n",
    "        sentences = [line.strip() for line in f.readlines()]  # Get list of sentences from given file\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = \"<s> \" * max(1, N - 1) + sentences[i].replace(\"\\n\", \"\") + \" </s>\" # Add start and end of sentence tag\n",
    "\n",
    "    tokens = ' '.join(sentences).split(\" \")  # Get list of individual tokens\n",
    "    \n",
    "    dictionary = dict() # store dictionary of all words along with their counts\n",
    "    for token in tokens:  # Count number of occurences of each token\n",
    "        if token in dictionary:\n",
    "            dictionary[token] += 1\n",
    "        else:\n",
    "            dictionary[token] = 1\n",
    "\n",
    "    for i in range(len(tokens)):  # Mark tokens with single occurrence with <UNK>\n",
    "        if dictionary[tokens[i]] == 1 and tokens[i] != \"<UNK>\":\n",
    "            del dictionary[tokens[i]]\n",
    "            tokens[i] = \"<UNK>\"\n",
    "            if \"<UNK>\" in dictionary:\n",
    "                dictionary[\"<UNK>\"] += 1\n",
    "            else:\n",
    "                dictionary[\"<UNK>\"] = 1\n",
    "\n",
    "    return dictionary, tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_N_gram_counts(tokens, N):\n",
    "    N_gram_counts = dict() # stores N-grams with their counts\n",
    "   \n",
    "    for i in range(len(tokens)-N+1):\n",
    "        N_gram = tuple(tokens[i:i+N]) # the tuple of N tokens is used as key in the dictionary of N-grams\n",
    "        if N_gram in N_gram_counts:\n",
    "            N_gram_counts[N_gram] += 1\n",
    "        else:\n",
    "            N_gram_counts[N_gram] = 1\n",
    "\n",
    "    return N_gram_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(tokens, N, dictionary):\n",
    "    N_gram_counts = get_N_gram_counts(tokens, N) # stores N-grams with their counts\n",
    "    N_1_gram_counts = get_N_gram_counts(tokens, N-1) # stores (N-1)-grams with their counts\n",
    "\n",
    "    distribution = {} # stores probability distribution of N-grams in the corpus\n",
    "    for key,value in N_gram_counts.items():\n",
    "        distribution[key] = (value+1)/(N_1_gram_counts[key[:-1]] + len(dictionary))\n",
    "\n",
    "    return distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(tokens, N, dictionary):\n",
    "    distribution = None\n",
    "    if N == 1:\n",
    "        distribution = {(key,): value / len(tokens)\n",
    "                        for key, value in dictionary.items()}\n",
    "    else:\n",
    "        distribution = laplace_smoothing(tokens, N, dictionary)\n",
    "\n",
    "    return distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log,exp\n",
    "from itertools import product\n",
    "\n",
    "def handle_oov_tokens(N_gram, model, N):\n",
    "    masks  = list(reversed(list(product((0,1), repeat=N)))) # bitmask to mark whether a token should be considered or replaced with <UNK>\n",
    "    for mask in masks:\n",
    "        modified_N_gram = []\n",
    "        for i in range(N):\n",
    "            if mask[i]==1:\n",
    "                modified_N_gram.append(N_gram[i])\n",
    "            else:\n",
    "                modified_N_gram.append(\"<UNK>\")\n",
    "        modified_N_gram = tuple(modified_N_gram)\n",
    "        if modified_N_gram in model:\n",
    "            return modified_N_gram\n",
    "\n",
    "\n",
    "def compute_perplexity(N, dictionary, model):\n",
    "    _,test_tokens = preprocess(\"test.txt\", N) # ignore dictionary of test corpus and get the tokens\n",
    "\n",
    "    perplexity = 0\n",
    "    for i in range(len(test_tokens)-N+1):\n",
    "        N_gram = tuple(test_tokens[i:i+N])\n",
    "        modified_N_gram = handle_oov_tokens(N_gram, model, N)\n",
    "        probability = model[modified_N_gram]\n",
    "        perplexity -= log(probability)\n",
    "    perplexity *= 1/len(test_tokens)\n",
    "    perplexity = exp(perplexity)\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(sentence, N, k, model, min_len, max_len):\n",
    "    sentence = \"<s> \" * max(1, N-1) + sentence\n",
    "    sentence = sentence.strip()\n",
    "    sentence = sentence.split(' ')\n",
    "    for i in range(k):\n",
    "        cur_sentence = sentence[:]\n",
    "        log_probability = 0\n",
    "        while cur_sentence[-1] != \"</s>\":\n",
    "            prev_tokens = tuple([]) if N == 1 else tuple(cur_sentence[-N+1:])\n",
    "            \n",
    "            possible_tokens = ((N_gram[-1],prob) for N_gram,prob in model.items() if N_gram[:-1]==prev_tokens)\n",
    "            \n",
    "            not_allowed = [\"<UNK>\"] + ([\"</s>\"] if len(cur_sentence)<=min_len else []) + cur_sentence\n",
    "            possible_tokens = filter(lambda token: token[0] not in not_allowed, possible_tokens)\n",
    "            possible_tokens = sorted(possible_tokens, key=lambda token: token[1], reverse=True)\n",
    "            next_token, next_token_prob = \"</s>\", 1\n",
    "            \n",
    "            if len(possible_tokens) != 0:\n",
    "                next_token, next_token_prob = possible_tokens[0 if len(cur_sentence)!=len(sentence) else i]\n",
    "            \n",
    "            cur_sentence.append(next_token)\n",
    "            log_probability += log(next_token_prob)\n",
    "            \n",
    "            if len(cur_sentence) == max_len:\n",
    "                cur_sentence.append(\"</s>\")\n",
    "        len_cur_sentence = len(cur_sentence)\n",
    "        cur_sentence = ' '.join(cur_sentence)\n",
    "        print(cur_sentence,\"[\", exp(-log_probability/len_cur_sentence), \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for N to be entered ...\n",
      "Preprocessing of training file started ...\n",
      "Preprcessing of training file completed\n",
      "Dictionary size = 23505\n",
      "Model training started for N = 3  ...\n",
      "Model training completed\n",
      "Computing perplexity for test set ...\n",
      "Perplexity of test set = 1412.184231933923\n",
      "Waiting for k to be entered ...\n",
      "Waiting for incomplete sentence to be entered (press enter for empty sentence) ...\n",
      "Generating top 10 sentence along with their log(probability) values ...\n",
      "<s> <s> i think the merger is expected to be a long term debt </s> [ 203.6190007918379 ]\n",
      "<s> <s> i dont think the merger is expected to be a long term debt </s> [ 238.22483750124556 ]\n",
      "<s> <s> i am sure that the us agriculture department said </s> [ 131.53824124763625 ]\n",
      "<s> <s> i would not be able to issue a bond on april 1 </s> [ 358.7123459492231 ]\n",
      "<s> <s> i believe the fed said that if they are not yet been reached </s> [ 483.26753854129305 ]\n",
      "<s> <s> i expect to see the recent paris meeting and noted that a further decline in january </s> [ 799.8123971743308 ]\n",
      "<s> <s> i have no comment on the new york stock exchange </s> [ 178.32772457173647 ]\n",
      "<s> <s> i do not want to be a long term debt </s> [ 234.48735629498137 ]\n",
      "<s> <s> i hope the foreign exchange reserves to more than one billion dlrs in cash and notes </s> [ 401.21125432457023 ]\n",
      "<s> <s> i cant put another name on it to be a long term debt </s> [ 786.3115088595723 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Waiting for N to be entered ...\")\n",
    "N = int(input(\"Enter N of model : \"))\n",
    "\n",
    "print(\"Preprocessing of training file started ...\")\n",
    "dictionary, tokens = preprocess(\"train.txt\", N)\n",
    "print(\"Preprcessing of training file completed\\nDictionary size =\", len(dictionary))\n",
    "\n",
    "print(\"Model training started for N =\", N, \" ...\")\n",
    "model = build_model(tokens, N, dictionary)\n",
    "print(\"Model training completed\")\n",
    "\n",
    "print(\"Computing perplexity for test set ...\")\n",
    "perplexity = compute_perplexity(N, dictionary, model)\n",
    "print(\"Perplexity of test set =\", perplexity)\n",
    "\n",
    "min_len = 10 # minimum length of generated sentence (can be less if oov tokens are given)\n",
    "max_len = 20 # maximum length of generated sentence\n",
    "print(\"Waiting for k to be entered ...\")\n",
    "k = int(input(\"Enter number of sentences to generate : \"))\n",
    "print(\"Waiting for incomplete sentence to be entered (press enter for empty sentence) ...\")\n",
    "sentence = input(\"Enter incomplete sentence : \")\n",
    "print(\"Generating top\", k, \"sentence along with their log(probability) values ...\")\n",
    "generate_sentence(sentence, N, k, model, min_len, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
